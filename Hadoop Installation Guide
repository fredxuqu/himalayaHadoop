Hadoop的环境搭建
	ubuntu14
	Hadoop2.7.6
	JDK1.8

	a.	设置网络为ANT模式
	b.	克隆虚拟机
	c.	在/etc/sudoers中将普通用户加入sudo
		fred ALL=(ALL) ALL

	d.	修改/etc/hostname，修改主机的名称，与节点名称一致（node1，node2，node3等，根据机器的配置而定）

	e.	修改/etc/hosts文件，必须注释掉 127.0.0.1 ubuntu这一行
		127.0.0.1       localhost
		#127.0.0.1       node101
		192.168.179.101 node101
		192.168.179.102 node102
		192.168.179.103 node103
		192.168.179.104 node104
		192.168.179.105 node105
	
	f.	修改/etc/network/interfaces，配置静态IP
		auto eth0
		iface eth0 inet static
		address 192.168.179.101
		netmask 255.255.255.0
		gateway 192.168.179.2
		broadcast 192.168.179.255
		dns-nameserver 114.114.114.114

	g.	修改/etc/reslov.conf，配置DNS，使用电信的DNS服务器114.114.114.114
		nameserver 8.8.8.8
		nameserver 114.114.114.114

	h.	重启networking
		/etc/init.d/networking restart

	i.	修改主机名
		hostname 
		vi /etc/hostname

	j.	关闭防火墙
		ufw status/disable/enable

	k.	在/opt 下创建目录
		mkdir module
		mkdir software
		chown fred:fred /module /software

	l.	安装JDK
		下载： http://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-8u181-linux-x64.tar.gz
		解压 tar -xvf jdk-8u181-linux-x64.tar.gz 
		配置环境变量  vi /etc/profile
		export JAVA_HOME=/usr/local/java
		export PATH=$PATH:$JAVA_HOME/bin
		
	m.	安装zookeeper	
		下载 zookeeper-3.4.8.tar.gz
		解压
		配置zoo.cfg
			tickTime=2000
			initLimit=10
			syncLimit=5
			dataDir=/opt/workspaces/zookeeper/data
			clientPort=2181
			server.1=node101:2888:3888
			server.2=node102:2888:3888
			server.3=node103:2888:3888
			
		修改zkEnv.sh，修改里面的日志文件位置
		
		配置zookeeper的环境变量
			export ZK_HOME=/opt/workspaces/zookeeper
			export PATH=$PATH:$ZK_HOME/bin
		
		启停zookeeper服务，查看节点状态
		zkServer.sh start/stop/status
		
	Hadoop集群
		1.	下载：http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.7.6/hadoop-2.7.6.tar.gz
		解压：tar -xvf hadoop-2.7.6.tar.gz 
		配置hadoop-env.sh，修改JAVA_HOME

		2.	添加Hadoop环境变量
		export HADOOP_HOME=/opt/workspaces/hadoop
		export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
		
		创建输入文件的目录
		mkdir /opt/workspaces/hadoop/input
		cp /opt/workspaces/hadoop/etc/hadoop/*.xml /opt/workspaces/hadoop/input
		
		使用官方提供的测试用例测试：
		/opt/workspaces/hadoop/bin/hadoop jar /opt/workspaces/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar grep input output 'dfs[a-z.]+'
		bin/hadoop jar /opt/workspaces/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount wcinput wcoutput
	
	伪分布式部署（HDFS）
		http://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-common/SingleCluster.html
		1. core-site.xml
			<property>
		        <name>fs.defaultFS</name>
		        <value>hdfs://node100:9000</value>
		    </property>
		    <property>
		        <name>hadoop.tmp.dir</name>
		        <value>/opt/workspaces/hadoop/data/tmp</value>
		    </property>
		2. hdfs-site.xml
			<property>
		        <name>dfs.replication</name>
		        <value>1</value>
		    </property>
		3. format namenode
			bin/hdfs namenode -format
			
		4. 启动namenode
			sbin/hadoop-daemon.sh start namenode
		
		5. 启动datanode
			sbin/hadoop-daemon.sh start datanode
			
		6. 通过web页面查看
			http://192.168.179.100:50070/
			
		7. 创建文件夹
			bin/hdfs dfs -mkdir -p /usr/fred/input
			bin/hdfs dfs -put wcinput/wc.input /usr/fred/input
			
		8. 测试
			bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount /usr/fred/input /usr/fred/output
		
		9. 查看运行结果
			bin/hdfs dfs -cat /usr/fred/wcoutput/p*
			
	伪分布式部署（YARN）
		mapred-site.xml
			<property>
		        <name>mapreduce.framework.name</name>
		        <value>yarn</value>
		    </property>
		
		yarn-site.xml
			<property>
		        <name>yarn.nodemanager.aux-services</name>
		        <value>mapreduce_shuffle</value>
		    </property>
		
		    <property>
		        <name>yarn.resourcemanager.hostname</name>
		        <value>node100</value>
		    </property>
		    
		启动namenode和datanode
			sbin/hadoop-daemon.sh start namenode
			sbin/hadoop-daemon.sh start datanode
			
		启动resourcemanager 和 nodemananger
			sbin/yarn-daemon.sh start resourcemanager
			sbin/yarn-daemon.sh start nodemanager
			
		检验
			http://192.168.179.100:8088
			
		删除output
		
		测试 
			bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount /usr/fred/input /usr/fred/output
			
		查看结果
			bin/hdfs dfs -cat /usr/fred/output/p*
			
	完全分布式部署
		3. config Login without authentication
			a. generate private key and public key by
				ssh-keygen -t rsa 
			b. copy public key to remote server by 
				ssh-copy-id -i ~/.ssh/id_rsa.pub fred@192.168.179.102
				
		4. config zookeeper server
						
		5. Config HDFS High Availability Using the Quorum Journal Manager
			a. hdfs_site.xml
				<property>
				  <name>dfs.nameservices</name>
				  <value>himalaya</value>
				</property>			
				<property>
				  <name>dfs.ha.namenodes.himalaya</name>
				  <value>nn1,nn2</value>
				</property>			
				<property>
				  <name>dfs.namenode.rpc-address.himalaya.nn1</name>
				  <value>node1:8020</value>
				</property>
				<property>
				  <name>dfs.namenode.rpc-address.himalaya.nn2</name>
				  <value>node2:8020</value>
				</property>			
				<property>
				  <name>dfs.namenode.http-address.himalaya.nn1</name>
				  <value>node1:50070</value>
				</property>
				<property>
				  <name>dfs.namenode.http-address.himalaya.nn2</name>
				  <value>node2:50070</value>
				</property>			
				<property>
				  <name>dfs.namenode.shared.edits.dir</name>
				  <value>qjournal://node1:8485;node2:8485;node3:8485/himalaya</value>
				</property>			
				<property>
				  <name>dfs.client.failover.proxy.provider.himalaya</name>
				  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
				</property>			
				<property>
				  <name>dfs.ha.fencing.methods</name>
				  <value>sshfence</value>
				</property>			
				<property>
				  <name>dfs.ha.fencing.ssh.private-key-files</name>
				  <value>/home/fred/.ssh/id_rsa</value>
				</property>			
				<property>
				  <name>dfs.journalnode.edits.dir</name>
				  <value>/home/fred/hadoop/jn/data</value>
				</property>			
				<property>
				  <name>dfs.ha.automatic-failover.enabled</name>
				  <value>true</value>
				</property>
			
			b. core-site.xml
				<property>
				  <name>fs.defaultFS</name>
				  <value>hdfs://himalaya</value>
				</property>			
				<property>
				  <name>ha.zookeeper.quorum</name>
				  <value>node1:2181,node2:2181,node3:2181</value>
				</property>			
				<property>
				  <name>hadoop.tmp.dir</name>
				  <value>/home/fred/hadoop/tmp</value>
				</property>
				
			c. config slaves
				node1
				node2
				node3
				
			d.	if os is ubuntu, change /etc/hosts, comment 127.0.1.1
		
		5. start up all server	
			a.  start zookeeper
				zkServer.sh start
			
			b. 	start hadoop server as journalnode, execute below command on each machine
				hadoop/sbin/hadoop-daemon.sh start journalnode
			
			c. 	format a namenode
				hadoop/bin/hdfs namenode -format
				attention : Please delete all files under hadoop.tmp.dir, /tmp and zookeeper/data/version-2
			
				original file will be found under tmp/dfs/name/current
				
			d.	start a namenode which is formatted.
				sbin/hadoop-daemon.sh start namenode
			
			e.	then execute below command on unformatted namenode server to format namenode
				bin/hdfs namenode -bootstrapStandby
				check tmp/dfs/name/current
				
				start current namenode server
				sbin/hadoop-daemon.sh start namenode
			
			f. 	format zkfc
				hadoop/bin/hdfs zkfc -formatZK
			
			e.	stop all hadoop service
				hadoop/sbin/stop-dfs.sh
			
				if system can't find JAVA_HOME, identify JAVA_HOME in hadoop-env.sh 
			
			f.	start up all service
				hadoop/sbin/start-dfs.sh
				
		6. create hdfs folder and upload file
			./hdfs dfs -mkdir -p /usr/input/temp
			
			./hdfs dfs -put /home/fred/data.txt /usr/input/temp
					
		7.	config mapreduce
			mapred-site.xml		
			<property>
				<name>mapreduce.framework.name</name>
				<value>yarn</value>
			</property>
			
			yarn-site.xml
			<property>
				<name>yarn.nodemanager.hostname</name>
				<value>node1</value>
			</property>
			<property>
				<name>yarn.nodemanager.aux-services</name>
				<value>mapreduce_shuffle</value>
			</property>
			<property>
				<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
				<value>org.apache.hadoop.mapred.ShuffleHandler</value>
			</property>
			













	
	
	
	